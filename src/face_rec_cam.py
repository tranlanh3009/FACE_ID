from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
# Disable TensorFlow 2.x behavior to use TensorFlow 1.x style
tf.compat.v1.disable_eager_execution()

from imutils.video import VideoStream

import argparse
import facenet
import imutils
import os
import sys
import math
import pickle
import align.detect_face
import numpy as np
import cv2
import collections
from sklearn.svm import SVC


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--path', help='Path of the video you want to test on.', default=0)
    args = parser.parse_args()

    MINSIZE = 20
    THRESHOLD = [0.6, 0.7, 0.7]
    FACTOR = 0.709
    IMAGE_SIZE = 182
    INPUT_IMAGE_SIZE = 160
    VIDEO_PATH = args.path
    FACENET_MODEL_PATH = 'Models/20180402-114759.pb'
    
    # C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c
    CONFIDENCE_THRESHOLD = 0.7   # Ng∆∞·ª°ng tin c·∫≠y
    MIN_FACE_SIZE = 40          # K√≠ch th∆∞·ªõc m·∫∑t t·ªëi thi·ªÉu (pixels)

    # Load The Custom Classifier - ∆∞u ti√™n model c·∫£i thi·ªán
    classifier_paths = [
        'Models/improved_facemodel.pkl',  # Model m·ªõi
        'Models/facemodel.pkl'            # Model c≈© backup
    ]
    
    model = None
    class_names = None
    
    for path in classifier_paths:
        try:
            with open(path, 'rb') as file:
                model, class_names = pickle.load(file)
            print(f"‚úÖ Loaded classifier: {path}")
            print(f"üìã Available classes: {class_names}")
            break
        except FileNotFoundError:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y: {path}")
        except Exception as e:
            print(f"‚ùå L·ªói load {path}: {e}")
    
    if model is None:
        print("‚ùå Kh√¥ng th·ªÉ load classifier!")
        return

    with tf.Graph().as_default():

        # Cai dat GPU neu co
        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)
        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))

        with sess.as_default():

            # Load the model
            print('Loading feature extraction model')
            facenet.load_model(FACENET_MODEL_PATH)

            # Get input and output tensors
            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")
            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")
            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")
            embedding_size = embeddings.get_shape()[1]

            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")

            people_detected = set()
            person_detected = collections.Counter()

            cap = VideoStream(src=0).start()

            while (True):
                frame = cap.read()
                if frame is None:
                    continue
                    
                frame = imutils.resize(frame, width=600)
                frame = cv2.flip(frame, 1)

                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)

                faces_found = bounding_boxes.shape[0]
                
                try:
                    if faces_found > 0:
                        det = bounding_boxes[:, 0:4]
                        bb = np.zeros((faces_found, 4), dtype=np.int32)
                        
                        for i in range(faces_found):
                            # T√≠nh to√°n bounding box
                            bb[i][0] = max(0, int(det[i][0]))
                            bb[i][1] = max(0, int(det[i][1]))
                            bb[i][2] = min(frame.shape[1], int(det[i][2]))
                            bb[i][3] = min(frame.shape[0], int(det[i][3]))
                            
                            # Ki·ªÉm tra k√≠ch th∆∞·ªõc khu√¥n m·∫∑t
                            face_height = bb[i][3] - bb[i][1]
                            face_width = bb[i][2] - bb[i][0]
                            
                            # Lu√¥n v·∫Ω bounding box
                            color = (0, 255, 0)  # M√†u xanh l√° m·∫∑c ƒë·ªãnh
                            name = "Unknown"
                            confidence_text = ""
                            
                            # Ch·ªâ th·ª±c hi·ªán nh·∫≠n di·ªán n·∫øu khu√¥n m·∫∑t ƒë·ªß l·ªõn
                            if face_width > MIN_FACE_SIZE and face_height > MIN_FACE_SIZE:
                                try:
                                    # C·∫Øt v√† x·ª≠ l√Ω khu√¥n m·∫∑t
                                    cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]
                                    
                                    if cropped.size > 0:
                                        scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),
                                                          interpolation=cv2.INTER_CUBIC)
                                        scaled = facenet.prewhiten(scaled)
                                        scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)
                                        feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}
                                        emb_array = sess.run(embeddings, feed_dict=feed_dict)

                                        predictions = model.predict_proba(emb_array)
                                        best_class_indices = np.argmax(predictions, axis=1)
                                        best_class_probabilities = predictions[
                                            np.arange(len(best_class_indices)), best_class_indices]
                                        best_name = class_names[best_class_indices[0]]
                                        confidence = best_class_probabilities[0]
                                        
                                        print(f"üéØ D·ª± ƒëo√°n: {best_name} (confidence: {confidence:.4f})")
                                        
                                        # Ki·ªÉm tra ƒë·ªô tin c·∫≠y
                                        if confidence > CONFIDENCE_THRESHOLD:
                                            name = best_name
                                            color = (0, 255, 0)  # Xanh l√° cho nh·∫≠n di·ªán th√†nh c√¥ng
                                            confidence_text = f" ({confidence:.2f})"
                                            person_detected[best_name] += 1
                                        else:
                                            name = "Unknown"
                                            color = (0, 0, 255)  # ƒê·ªè cho kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c
                                            confidence_text = f" ({confidence:.2f})"
                                            
                                except Exception as e:
                                    print(f"L·ªói nh·∫≠n di·ªán: {e}")
                                    name = "Error"
                                    color = (128, 128, 128)  # X√°m cho l·ªói
                            else:
                                name = "Too small"
                                color = (255, 0, 0)  # Xanh d∆∞∆°ng cho khu√¥n m·∫∑t qu√° nh·ªè
                            
                            # V·∫º BOUNDING BOX V√Ä T√äN (LU√îN LU√îN)
                            cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), color, 2)
                            
                            # V·∫Ω t√™n v√† ƒë·ªô tin c·∫≠y
                            text_x = bb[i][0]
                            text_y = bb[i][1] - 10 if bb[i][1] > 30 else bb[i][3] + 20
                            
                            cv2.putText(frame, name + confidence_text, (text_x, text_y), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)
                            
                        # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng khu√¥n m·∫∑t ph√°t hi·ªán
                        cv2.putText(frame, f"Faces detected: {faces_found}", (10, 30), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    else:
                        # Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o
                        cv2.putText(frame, "No face detected", (10, 30), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

                except Exception as e:
                    print(f"Error in face recognition: {e}")
                    cv2.putText(frame, f"Error: {str(e)}", (10, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)

                # Hi·ªÉn th·ªã th√¥ng tin h·ªá th·ªëng
                cv2.putText(frame, f"Confidence threshold: {CONFIDENCE_THRESHOLD}", (10, frame.shape[0] - 40), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                cv2.putText(frame, "Press 'q' to quit", (10, frame.shape[0] - 20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

                cv2.imshow('Face Recognition - Improved', frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

            cap.stop()
            cv2.destroyAllWindows()


main()
